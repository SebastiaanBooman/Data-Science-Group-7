{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Vector Auto-Regressive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alles is basically hiervan gestolen: https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes sure Pandas keeps it mouth shut\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Before we do anything, we first import our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwt = pd.read_excel('../../Data/pwt1001.xlsx',\n",
    "                    sheet_name = 'Data',\n",
    "                    parse_dates = ['year'],\n",
    "                    index_col = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we:\n",
    "- Filter out the country we will make the prediction for\n",
    "- Select the columns we use in the calculation\n",
    "- Drop N/A values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = 'cgdpo'\n",
    "country_code = 'USA'\n",
    "\n",
    "# Filter out the relevant country\n",
    "df = pwt[pwt['countrycode'] == country_code]\n",
    "country_name = df['country'][0]\n",
    "\n",
    "# Filter out the variables we need and drop N/A values\n",
    "df = df[[gdp, 'ccon', 'rdana']]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Causation\n",
    "Granger's causality test is used to determine whether one time series influence another, i.e. it tests if there is causation or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def causation_test(data: pd.DataFrame, verbose = False) -> bool:\n",
    "    maxlag = round(12 * pow(len(df) / 100, 1/4))\n",
    "    alpha = 0.05\n",
    "    results = pd.DataFrame([], columns = data.columns, index = data.columns)\n",
    "    causation = True\n",
    "\n",
    "    # TODO: find out whether we need to have causation between all variables,\n",
    "    #       or only for the variable we want, the rgdpna\n",
    "    for col in results.columns:\n",
    "        for row in results.index:\n",
    "            if row == col:\n",
    "                results.loc[row, col] = 1\n",
    "                continue\n",
    "\n",
    "            result = grangercausalitytests(\n",
    "                data[[row, col]],\n",
    "                maxlag   = maxlag,\n",
    "                addconst = True,\n",
    "                verbose  = False\n",
    "            )\n",
    "\n",
    "            p_values = [round(result[i + 1][0]['ssr_chi2test'][1], 3) for i in range(maxlag)]\n",
    "            p_value = np.min(p_values)\n",
    "\n",
    "            results.loc[row, col] = p_value\n",
    "\n",
    "            if p_value >= alpha:\n",
    "                causation = False\n",
    "\n",
    "    if verbose:\n",
    "        display(results)\n",
    "\n",
    "    return causation\n",
    "\n",
    "if causation_test(df, True):\n",
    "    print('Causation is present between all variables')\n",
    "else:\n",
    "    print('Causation is not present for all variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Cointegration\n",
    "Cointegration means there is a long-term tendency for two or more time series to move together.\n",
    "\n",
    "There are multiple ways to test for cointegration, like Engle-Granger and Johansen. While the Engle-Granger test is more simple, the Johansen test allows for multiple cointegrated relationships to be tested. Johansen is therefore the one that is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(data: pd.DataFrame, verbose = False) -> bool:\n",
    "    results = pd.DataFrame()\n",
    "    cointegration = True\n",
    "\n",
    "    result = coint_johansen(\n",
    "        data,\n",
    "        det_order = -1, # No deterministic terms\n",
    "        k_ar_diff = 5 # TODO not entirely sure what value we should use here\n",
    "    )\n",
    "\n",
    "    for name, trace, cv95 in zip(df.columns, result.trace_stat, result.cvt[:,1]):\n",
    "        results[name] = {\n",
    "            'Trace':                round(trace, 3),\n",
    "            'Critical Value (95%)': round(cv95, 3),\n",
    "            'Significant':          'Yes' if trace > cv95 else 'No'\n",
    "        }\n",
    "\n",
    "        if trace <= cv95:\n",
    "            cointegration = False\n",
    "    \n",
    "    if verbose:\n",
    "        display(results.transpose())\n",
    "\n",
    "    return cointegration\n",
    "\n",
    "if cointegration_test(df, True):\n",
    "    print('A significant relationship is present between all variables')\n",
    "else:\n",
    "    print('A significant relationship is not present for all variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "Now we split the data into a test, and training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(len(df) * 0.75)\n",
    "df_train = df[:train_length]\n",
    "df_test = df[train_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "Now a _Augmented Dickey-Fuller (ADF)_ test is performed to check if the data is stationary.\n",
    "\n",
    "When data is stationary, its properties such as variance and mean, are constant over time. For VAR to forecast meaningful results, it is required that the data that is put in is stationary. \n",
    "\n",
    "There are also other methods of determining whether a series is stationary or not, but ADF seems to be the most popular one.\n",
    "\n",
    "A significance level (alpha) of __5%__ is used to determine whether or not a series is stationary or not.\n",
    "\n",
    "If any series is not stationary, all series are differenced again until they all are stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def stationarity_test(data: pd.DataFrame, verbose = False) -> bool:\n",
    "    maxlag = round(12 * pow(len(df) / 100, 1/4))\n",
    "    alpha = 0.05\n",
    "    results = pd.DataFrame()\n",
    "    stationary = True\n",
    "\n",
    "    for name, series in data.items():\n",
    "        result = adfuller(\n",
    "            series,\n",
    "            maxlag     = maxlag,\n",
    "            regression = 'c', # Constant regression\n",
    "            autolag    = 'AIC', \n",
    "            store      = False,\n",
    "            regresults = False\n",
    "        )\n",
    "\n",
    "        results[name] = {\n",
    "            'P-Value':     round(result[1], 3),\n",
    "            'Number lags': result[2],\n",
    "            'Stationary':  'Yes' if result[1] <= alpha else 'No'\n",
    "        }\n",
    "\n",
    "        if result[1] > alpha:\n",
    "            stationary = False\n",
    "\n",
    "    if verbose:\n",
    "        display(results.transpose())\n",
    "\n",
    "    return stationary\n",
    "\n",
    "df_diff = df_train\n",
    "diff_pass = 1\n",
    "while stationarity_test(df_diff, True) == False:\n",
    "    print(f'One or more series are non-stationary, differencing... (pass = {diff_pass})')\n",
    "    df_diff = df_diff.diff().dropna()\n",
    "    diff_pass += 1\n",
    "\n",
    "    if diff_pass > 10:\n",
    "        print('Unable to make all series stationary')\n",
    "        break\n",
    "else:\n",
    "    print('All series are stationary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Lag Order Selection\n",
    "\n",
    "The lag order refers to how far the VAR model looks back in the time series.\n",
    "\n",
    "Determining the optimal lag order will greatly influence the results of the prediction. Besides manually selecting the lag order, there are several statistical methods that select the lag order, such as AIC, BIC, FPE and HQIC.\n",
    "\n",
    "Here, the lag order is automatically chosen based on the HQIC method. QHIC is a combination of both AIC and BIC.\n",
    "AIC is the most simple method, but is prone to overfitting.\n",
    "BIC is more conservative, and is less likely to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "model = VAR(df_diff, freq = df_diff.index.inferred_freq)\n",
    "maxlag = model.select_order().selected_orders['bic']\n",
    "\n",
    "print(model.select_order().selected_orders)\n",
    "print(f'Max. lag = {maxlag}')\n",
    "maxlag = 1 # FIXME: what is the right max lag???\n",
    "\n",
    "results = model.fit(\n",
    "    maxlags = maxlag,\n",
    "    method  = 'ols',\n",
    "    ic      = None,\n",
    "    trend   = 'c' # Add constant\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAR_parameter_selection import var_parameter_search\n",
    "params = var_parameter_search(model=model,\n",
    "                              target_column=gdp,\n",
    "                              df_train=df_train,\n",
    "                              df_diff=df_diff,\n",
    "                              df_test=df_test,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Serial Correlation of Residuals\n",
    "\n",
    "TODO explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "out = durbin_watson(results.resid)\n",
    "\n",
    "def adjust(val, length= 6): return str(val).ljust(length)\n",
    "for col, val in zip(df.columns, out):\n",
    "    print(adjust(col), ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "Now the model will make a prediction for the period of the test data based on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = len(df_test)\n",
    "alpha = 0.05\n",
    "\n",
    "# Do the forecast\n",
    "mid, lower, upper = results.forecast_interval(df_diff.values[-maxlag:], steps = horizon, alpha = alpha)\n",
    "\n",
    "# Put it into a dataframe\n",
    "df_forecast_diff = pd.DataFrame(mid, columns = df_diff.columns, index = df_test.iloc[:horizon].index)\n",
    "df_lower_error_diff = pd.DataFrame(lower, columns = df_diff.columns, index = df_test.iloc[:horizon].index)\n",
    "df_upper_error_diff = pd.DataFrame(upper, columns = df_diff.columns, index = df_test.iloc[:horizon].index)\n",
    "\n",
    "# Reverse the differencing\n",
    "df_forecast = df_forecast_diff.cumsum() + df_train[df_train.index < df_forecast_diff.index[0]].iloc[-1]\n",
    "df_lower_error = df_lower_error_diff.cumsum() + df_train[df_train.index < df_lower_error_diff.index[0]].iloc[-1]\n",
    "df_upper_error = df_upper_error_diff.cumsum() + df_train[df_train.index < df_upper_error_diff.index[0]].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rmse = mean_squared_error(df_forecast[gdp], df_test[gdp].values[:horizon], squared = False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(df_train[-train_length:][gdp], color='black', label='Train')\n",
    "ax.plot(df_test[:horizon][gdp], color='tab:blue', label = 'Test')\n",
    "ax.plot(df_forecast[gdp], color = 'tab:orange', label = 'Forecast')\n",
    "ax.fill_between(df_forecast[gdp].index, df_lower_error[gdp].values, df_upper_error[gdp].values, color = 'bisque', label='95% CI')\n",
    "\n",
    "ax.xaxis.set_ticks_position('none')\n",
    "ax.yaxis.set_ticks_position('none')\n",
    "ax.spines['top'].set_alpha(0)\n",
    "ax.tick_params(labelsize = 6)\n",
    "ax.set_title(f'{country_name} GDP')\n",
    "ax.set_title(f'RMSE: {rmse}', loc = 'left', x = 0.04, y = 0.9, fontsize = 'small')\n",
    "ax.legend(loc = 'lower right')\n",
    "ax.spines[['right', 'top']].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
